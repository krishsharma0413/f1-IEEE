{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_15552\\3123182197.py:2: DtypeWarning: Columns (4,13,14,16,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  traindf=pd.read_csv(\"./data/train.csv\")\n",
      "C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_15552\\3123182197.py:3: DtypeWarning: Columns (13,16,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  validationdf = pd.read_csv(\"./data/validation.csv\")\n"
     ]
    }
   ],
   "source": [
    "# all data\n",
    "traindf=pd.read_csv(\"./data/train.csv\")\n",
    "validationdf = pd.read_csv(\"./data/validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.replace('\\\\N', None, inplace=True)\n",
    "\n",
    "# dropping values that are not neeeded\n",
    "df=traindf.drop(['time_x', 'timetaken_in_millisec', 'fastestLap', 'rank', 'fastestLapTime', 'max_speed', 'time_y', 'url_x', 'fp1_date', 'fp1_time', 'fp2_date'\n",
    "            , 'fp2_time', 'fp3_date', 'fp3_time', 'quali_date', 'quali_time', 'sprint_date', 'sprint_time', 'driver_num', 'driver_code', 'url_y'\n",
    "             ,\"number\", \"position_x\", 'nationality_y', 'url', 'status', 'positionText_y', 'constructorRef', 'forename', 'surname', 'driverRef', 'positionText_x', 'nationality'], axis=1)\n",
    "\n",
    "\n",
    "#converting the dob and date column into years to determine the age of the driver\n",
    "df['dob'] = pd.to_datetime(df['dob'])\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['time_difference'] = df['date'] - df['dob']\n",
    "df['years'] = (df['time_difference'].dt.days / 365.25).astype(int)\n",
    "\n",
    "df = df.drop(['dob', 'date', 'time_difference'], axis=1)\n",
    "\n",
    "# using frequency encoding for grand_prix and company\n",
    "frequency_grand_prix = df['grand_prix'].value_counts(normalize=True)\n",
    "df['grand_prix_encoded'] = df['grand_prix'].map(frequency_grand_prix)\n",
    "\n",
    "frequency_company = df['company'].value_counts(normalize=True)\n",
    "df['company_encoded'] = df['company'].map(frequency_company)\n",
    "\n",
    "df = df.drop(['grand_prix', 'company'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validationdf.replace('\\\\N', None, inplace=True)\n",
    "\n",
    "# dropping values that are not neeeded\n",
    "df2=validationdf.drop(['time_x', 'timetaken_in_millisec', 'fastestLap', 'rank', 'fastestLapTime', 'max_speed', 'time_y', 'url_x', 'fp1_date', 'fp1_time', 'fp2_date'\n",
    "            , 'fp2_time', 'fp3_date', 'fp3_time', 'quali_date', 'quali_time', 'sprint_date', 'sprint_time', 'driver_num', 'driver_code', 'url_y'\n",
    "             ,\"number\", \"position_x\", 'nationality_y', 'url', 'status', 'positionText_y', 'constructorRef', 'forename', 'surname', 'driverRef', 'positionText_x', 'nationality'], axis=1)\n",
    "\n",
    "\n",
    "#converting the dob and date column into years to determine the age of the driver\n",
    "df2['dob'] = pd.to_datetime(df2['dob'])\n",
    "df2['date'] = pd.to_datetime(df2['date'])\n",
    "df2['time_difference'] = df2['date'] - df2['dob']\n",
    "df2['years'] = (df2['time_difference'].dt.days / 365.25).astype(int)\n",
    "\n",
    "df2 = df2.drop(['dob', 'date', 'time_difference'], axis=1)\n",
    "\n",
    "# using frequency encoding for grand_prix and company\n",
    "frequency_grand_prix = df2['grand_prix'].value_counts(normalize=True)\n",
    "df2['grand_prix_encoded'] = df2['grand_prix'].map(frequency_grand_prix)\n",
    "\n",
    "frequency_company = df2['company'].value_counts(normalize=True)\n",
    "df2['company_encoded'] = df2['company'].map(frequency_company)\n",
    "\n",
    "df2 = df2.drop(['grand_prix', 'company'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['grand_prix_encoded'], axis=1)\n",
    "df2 = df2.drop(['grand_prix_encoded'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.drop(columns=['position', 'result_driver_standing'])\n",
    "y_train = df['position']\n",
    "train_id = df['result_driver_standing']\n",
    "\n",
    "X_test = df2.drop(columns=['position', 'result_driver_standing'])\n",
    "y_test = df2['position']\n",
    "test_id = df2['result_driver_standing']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.apply(pd.to_numeric)\n",
    "X_test.apply(pd.to_numeric)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(18, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 16)\n",
    "        self.fc4 = nn.Linear(16, 8)\n",
    "        self.fc5 = nn.Linear(8, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 18)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN().to(device)\n",
    "\n",
    "train_data = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "test_data = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "train_targets = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "test_targets = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        mse = nn.MSELoss()\n",
    "        return torch.sqrt(mse(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10  # Example: Train for a maximum of 100 epochs\n",
    "best_loss = float('inf')\n",
    "best_model_state = None\n",
    "patience = 5  # Stop if validation loss does not improve for 5 consecutive epochs\n",
    "counter = 0  # Counter to track epochs without improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 99021.6406\n",
      "Epoch [2/100], Loss: 20487.7051\n",
      "Epoch [3/100], Loss: 21677.5723\n",
      "Epoch [4/100], Loss: 8172.7744\n",
      "Epoch [5/100], Loss: 2578.7578\n",
      "Epoch [6/100], Loss: 240.9350\n",
      "Epoch [7/100], Loss: 191.2259\n",
      "Epoch [8/100], Loss: 168.6819\n",
      "Epoch [9/100], Loss: 151.0499\n",
      "Epoch [10/100], Loss: 139.1486\n",
      "Epoch [11/100], Loss: 136.4941\n",
      "Epoch [12/100], Loss: 143.9600\n",
      "Epoch [13/100], Loss: 152.9803\n",
      "Epoch [14/100], Loss: 158.5436\n",
      "Epoch [15/100], Loss: 159.1911\n",
      "Epoch [16/100], Loss: 189.8098\n",
      "Epoch [17/100], Loss: 157.9162\n",
      "Epoch [18/100], Loss: 154.9705\n",
      "Epoch [19/100], Loss: 149.6794\n",
      "Epoch [20/100], Loss: 143.6344\n",
      "Epoch [21/100], Loss: 138.3233\n",
      "Epoch [22/100], Loss: 134.8961\n",
      "Epoch [23/100], Loss: 134.0976\n",
      "Epoch [24/100], Loss: 135.6194\n",
      "Epoch [25/100], Loss: 138.3990\n",
      "Epoch [26/100], Loss: 140.6029\n",
      "Epoch [27/100], Loss: 140.6772\n",
      "Epoch [28/100], Loss: 137.7447\n",
      "Epoch [29/100], Loss: 132.2056\n",
      "Epoch [30/100], Loss: 126.6808\n",
      "Epoch [31/100], Loss: 124.7991\n",
      "Epoch [32/100], Loss: 126.9750\n",
      "Epoch [33/100], Loss: 128.5181\n",
      "Epoch [34/100], Loss: 126.1231\n",
      "Epoch [35/100], Loss: 121.5692\n",
      "Epoch [36/100], Loss: 118.0199\n",
      "Epoch [37/100], Loss: 117.2349\n",
      "Epoch [38/100], Loss: 117.3420\n",
      "Epoch [39/100], Loss: 116.7687\n",
      "Epoch [40/100], Loss: 115.0582\n",
      "Epoch [41/100], Loss: 112.6609\n",
      "Epoch [42/100], Loss: 110.8126\n",
      "Epoch [43/100], Loss: 109.8279\n",
      "Epoch [44/100], Loss: 110.0037\n",
      "Epoch [45/100], Loss: 109.5472\n",
      "Epoch [46/100], Loss: 108.2270\n",
      "Epoch [47/100], Loss: 107.0850\n",
      "Epoch [48/100], Loss: 106.5670\n",
      "Epoch [49/100], Loss: 106.5007\n",
      "Epoch [50/100], Loss: 106.3236\n",
      "Epoch [51/100], Loss: 105.8864\n",
      "Epoch [52/100], Loss: 105.6269\n",
      "Epoch [53/100], Loss: 105.5561\n",
      "Epoch [54/100], Loss: 105.2005\n",
      "Epoch [55/100], Loss: 104.2818\n",
      "Epoch [56/100], Loss: 103.4310\n",
      "Epoch [57/100], Loss: 103.1488\n",
      "Epoch [58/100], Loss: 102.7329\n",
      "Epoch [59/100], Loss: 102.0188\n",
      "Epoch [60/100], Loss: 101.1825\n",
      "Epoch [61/100], Loss: 100.3320\n",
      "Epoch [62/100], Loss: 99.5312\n",
      "Epoch [63/100], Loss: 98.8318\n",
      "Epoch [64/100], Loss: 98.0732\n",
      "Epoch [65/100], Loss: 97.3668\n",
      "Epoch [66/100], Loss: 97.1137\n",
      "Epoch [67/100], Loss: 96.9007\n",
      "Epoch [68/100], Loss: 96.2648\n",
      "Epoch [69/100], Loss: 95.3986\n",
      "Epoch [70/100], Loss: 94.5978\n",
      "Epoch [71/100], Loss: 93.9206\n",
      "Epoch [72/100], Loss: 93.2674\n",
      "Epoch [73/100], Loss: 92.5882\n",
      "Epoch [74/100], Loss: 91.8141\n",
      "Epoch [75/100], Loss: 90.7829\n",
      "Epoch [76/100], Loss: 89.8499\n",
      "Epoch [77/100], Loss: 89.2032\n",
      "Epoch [78/100], Loss: 88.2032\n",
      "Epoch [79/100], Loss: 87.6415\n",
      "Epoch [80/100], Loss: 87.5002\n",
      "Epoch [81/100], Loss: 86.9421\n",
      "Epoch [82/100], Loss: 86.9812\n",
      "Epoch [83/100], Loss: 86.3843\n",
      "Epoch [84/100], Loss: 86.3863\n",
      "Epoch [85/100], Loss: 86.2032\n",
      "Epoch [86/100], Loss: 86.0593\n",
      "Epoch [87/100], Loss: 86.0743\n",
      "Epoch [88/100], Loss: 85.8672\n",
      "Epoch [89/100], Loss: 86.0069\n",
      "Epoch [90/100], Loss: 85.7933\n",
      "Epoch [91/100], Loss: 85.8686\n",
      "Epoch [92/100], Loss: 85.5805\n",
      "Epoch [93/100], Loss: 85.5906\n",
      "Epoch [94/100], Loss: 85.3581\n",
      "Epoch [95/100], Loss: 85.3071\n",
      "Epoch [96/100], Loss: 85.0956\n",
      "Epoch [97/100], Loss: 84.9712\n",
      "Epoch [98/100], Loss: 84.8379\n",
      "Epoch [99/100], Loss: 84.6807\n",
      "Epoch [100/100], Loss: 84.5779\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()                         # Set the model to training mode\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model.forward(train_data)\n",
    "    \n",
    "    loss = criterion(outputs, train_targets)  # Compute RMSE loss for all output nodes\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 1 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on Validation Set: 7.8239\n"
     ]
    }
   ],
   "source": [
    "model.eval()                             # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_data)\n",
    "    rmse = math.sqrt(mean_squared_error(test_targets.cpu(), outputs.cpu()))\n",
    "    print(f'RMSE on Validation Set: {rmse:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
